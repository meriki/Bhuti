# -*- coding: utf-8 -*-
"""LSTM_HAri.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/0B3d6omb3GfWCbWZDS0RoS3lLeTVJUDdvMnNJaVIxZW5JcDlj
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Input, layers, Model,models
from tensorflow.keras.layers import Dense,  Dropout
#from keras.models import Mode

tf.__version__

# from google.colab import drive
# drive.mount('/content/drive')

#import os
import numpy as np
import pandas as pd

x_train = np.load('Bhuti-data/x_train.npy')
#x_train1_2 = np.load('drive/My Drive/bhuti_data/x_train1_2.npy')
#y_train1_2 = np.load('drive/My Drive/bhuti_data/y_train1_2.npy')
#x_train2_2 = np.load('drive/My Drive/bhuti_data/x_train2_2.npy')
#y_train2_2 = np.load('drive/My Drive/bhuti_data/y_train2_2.npy')

x_test = np.load('Bhuti-data/x_test.npy')

y_train = np.load('Bhuti-data/y_train.npy')

y_test = np.load('Bhuti-data/y_test.npy')

y_train = pd.get_dummies(y_train).values

y_test = pd.get_dummies(y_test).values

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import LSTM


# code for building an LSTM with 100 neurons and dropout. Runs for 50 epochs

time_steps = 200
n_features = 12

model = Sequential()
model.add(LSTM(100, return_sequences=False, input_shape=(time_steps, n_features)))
model.add(Dropout(0.5))
#model.add(LSTM(100)) dramatically worse results
model.add(Dense(5, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=16, epochs=1, verbose = 1,validation_split=0.33)
score = model.evaluate(x_test, y_test, batch_size=16)

plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['acc'],label = 'train_acc')
plt.plot(history.history['val_acc'],label = 'val_acc')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.show()

model.summary()

score = model.evaluate(x_test, y_test, batch_size=16)

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()

score

print("Accuracy: %.2f%%" % (score[1]*100))

