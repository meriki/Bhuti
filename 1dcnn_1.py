# -*- coding: utf-8 -*-
"""Clean_1DCNN_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19j-YHySSIV-0vNdbCPwFgC0EzFJ2nx1T
"""

!pip uninstall tensorflow

#pip install tensorflow
pip install tensorflow-gpu



import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Input, layers, Model,models
from tensorflow.keras.layers import Dense, Conv1D, Dropout, MaxPooling1D, GlobalAveragePooling1D,Flatten,BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint
#from keras.models import Model

tf.__version__

from google.colab import drive
drive.mount('/content/drive')

#import os
import numpy as np
import pandas as pd

x_train = np.load('drive/My Drive/Bhuti_val_data/x_train.npy')
#x_train1_2 = np.load('drive/My Drive/bhuti_data/x_train1_2.npy')
#y_train1_2 = np.load('drive/My Drive/bhuti_data/y_train1_2.npy')
#x_train2_2 = np.load('drive/My Drive/bhuti_data/x_train2_2.npy')
#y_train2_2 = np.load('drive/My Drive/bhuti_data/y_train2_2.npy')

x_test = np.load('drive/My Drive/Bhuti_val_data/x_test.npy')

y_train = np.load('drive/My Drive/Bhuti_val_data/y_train.npy')

y_test = np.load('drive/My Drive/Bhuti_val_data/y_test.npy')

y_val = np.load('drive/My Drive/Bhuti_val_data/y_val.npy')
x_val = np.load('drive/My Drive/Bhuti_val_data/x_val.npy')

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
print(x_val.shape)
print(y_val.shape)

y_train = pd.get_dummies(y_train).values
y_val = pd.get_dummies(y_val).values

y_test = pd.get_dummies(y_test).values

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)
print(x_val.shape)
print(y_val.shape)

CNNch = 12

# epch
ne = 20

modelC2 = models.Sequential()
#1
modelC2.add(Conv1D(filters=64, kernel_initializer=keras.initializers.glorot_uniform(seed=42),kernel_size=3,strides = 1, padding='same', activation='relu', 
                        input_shape=(200, 12)))
modelC2.add(MaxPooling1D(pool_size=2))
modelC2.add(BatchNormalization(momentum=0.9,axis=-1))
#2
modelC2.add(Conv1D(filters=64, kernel_initializer=keras.initializers.glorot_uniform(seed=42),kernel_size=3,strides = 1, padding='same', activation='relu'))
modelC2.add(MaxPooling1D(pool_size=2))
modelC2.add(BatchNormalization(momentum=0.9,axis=-1))
#3
modelC2.add(Conv1D(filters=64, kernel_initializer=keras.initializers.glorot_uniform(seed=42),kernel_size=3,strides = 1, padding='same', activation='relu'))
modelC2.add(MaxPooling1D(pool_size=2))
modelC2.add(Dropout(0.2))
modelC2.add(BatchNormalization(momentum=0.9,axis=-1))
#4
modelC2.add(Conv1D(filters=64, kernel_initializer=keras.initializers.glorot_uniform(seed=42),kernel_size=3,strides = 1, padding='same', activation='relu'))
modelC2.add(MaxPooling1D(pool_size=2))
modelC2.add(Dropout(0.2))
modelC2.add(BatchNormalization(momentum=0.9,axis=-1))
#5
modelC2.add(Conv1D(filters=64, kernel_initializer=keras.initializers.glorot_uniform(seed=42),kernel_size=3,strides = 1, padding='same', activation='relu'))
modelC2.add(MaxPooling1D(pool_size=2))
modelC2.add(Dropout(0.2))
modelC2.add(BatchNormalization(momentum=0.9,axis=-1))

modelC2.add(Flatten())
modelC2.add(Dense(50, activation='relu'))
modelC2.add(Dropout(0.2))
modelC2.add(Dense(5, activation='softmax'))

modelC2.summary()


# compile the model
modelC2.compile(loss='categorical_crossentropy', optimizer='rmsprop', 
                  metrics=['accuracy'])

# train the model
checkpointer = ModelCheckpoint(filepath='drive/My Drive/Models_Bhuti/CNNC2.weights.best.hdf5', verbose=1, 
                               save_best_only=True)

hist = modelC2.fit(x_train[:,:,0:CNNch], y_train, batch_size=32, epochs=ne,
          validation_data=(x_val[:,:,0:CNNch], y_val), callbacks=[checkpointer], 
          verbose=1, shuffle=True)

# load the weights that yielded the best validation accuracy
modelC2.load_weights('drive/My Drive/Models_Bhuti/CNNC2.weights.best.hdf5')

# evaluate and print test accuracy
score = modelC2.evaluate(x_test[:,:,0:CNNch], y_test, verbose=0)
print('\n', 'CNN Test accuracy:', score[1])

score = modelC2.evaluate(x_train[:,:,0:CNNch], y_train, verbose=0)
print('\n', 'CNN train accuracy:', score[1])

score = modelC2.evaluate(x_val[:,:,0:CNNch], y_val, verbose=0)
print('\n', 'CNN validation accuracy:', score[1])

# train the model
checkpointer = ModelCheckpoint(filepath='CNNC2.weights.best.hdf5', verbose=1, 
                               save_best_only=True)

hist = modelC2.fit(X_train[:,:,0:CNNch], Y_train, batch_size=32, epochs=ne,
          validation_data=(X_valid[:,:,0:CNNch], Y_valid), callbacks=[checkpointer], 
          verbose=1, shuffle=True)

# load the weights that yielded the best validation accuracy
modelC2.load_weights('CNNC2.weights.best.hdf5')

# evaluate and print test accuracy
score = modelC2.evaluate(X_test[:,:,0:CNNch], Y_test, verbose=0)
print('\n', 'CNN Test accuracy:', score[1])

score = modelC2.evaluate(X_train[:,:,0:CNNch], Y_train, verbose=0)
print('\n', 'CNN train accuracy:', score[1])

score = modelC2.evaluate(X_valid[:,:,0:CNNch], Y_valid, verbose=0)
print('\n', 'CNN validation accuracy:', score[1])